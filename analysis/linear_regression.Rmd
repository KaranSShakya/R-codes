---
title: "Linear Regression"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

This summarizes key concepts and directions for performing **linear regression**. Most of the steps are taken from Duke University's [Linear Regression and Modeling](https://www.coursera.org/learn/linear-regression-model/home/welcome) course on coursera.

***
```{r eval=T, echo=F, warning=F, results=F, message=F}
library(tidyverse)
library(ggplot2)
library(readxl)
library(broom)

cricket <- read_excel("data/cricket_temp.xlsx")
names(cricket)[1] <- "sound"
names(cricket)[2] <- "temp"
```
## Linear Regression

### 1. Correlation

* correlation is the strength of linear association

* correlation coefficients are sensitive to outliers

* R = cor(x,y). R-squared = (correlation)^2.

This is the correlation code for a table (x=temp, y=sound).
```{r eval=T, echo=T}
cor <- cricket %>% 
  summarise(r=cor(sound, temp)) %>% 
  pull(r)
cor
```
This is the scatterplot to see the points.
```{r eval=T, echo=T, message=F, warning=F}
ggplot(cricket, aes(x=temp, y=sound))+
  geom_point(alpha=0.5)+
  geom_smooth(method = "lm", se=F)
```

### 2. Residuals
Residuals are the difference between observed and predicted values. To visualize this we have used the **broom** package to test the residuals. 

* Residuals (errors) = observed - predicted

```{r eval=T, echo=T}
lm <- lm(sound~temp, data=cricket)
lm.table <- augment(lm) #can visualize all the residuals in a table form
```

ggplot(lm.table, aes(x=.fitted, y=.resid))+
  geom_point(alpha=0.5)

### 3. Least Square Lines







***