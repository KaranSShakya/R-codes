---
title: "Linear Regression"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

This summarizes key concepts and directions for performing **linear regression**. Most of the steps are taken from Duke University's [Linear Regression and Modeling](https://www.coursera.org/learn/linear-regression-model/home/welcome) course on coursera.

***
```{r eval=T, echo=F, warning=F, results=F, message=F}
library(tidyverse)
library(ggplot2)
library(readxl)
library(broom)
library(gridExtra)

cricket <- read_excel("data/cricket_temp.xlsx")
names(cricket)[1] <- "sound"
names(cricket)[2] <- "temp"
```
## Linear Regression

### 1. Correlation

* correlation is the strength of linear association

* correlation coefficients are sensitive to outliers

$R = cor(x,y). R^2 = (correlation)^2$

This is the correlation code for a table (x=temp, y=sound).
```{r eval=T, echo=T}
cor <- cricket %>% 
  summarise(r=cor(sound, temp)) %>% 
  pull(r)
cor
```
This is the scatterplot to see the points.
```{r eval=T, echo=T, message=F, warning=F}
ggplot(cricket, aes(x=temp, y=sound))+
  geom_point(alpha=0.5)+
  geom_smooth(method = "lm", se=F)
```

### 2. Residuals
Residuals are the difference between observed and predicted values. To visualize this we have used the **broom** package to test the residuals. 

$Residuals (errors) = observed - predicted$

```{r eval=T, echo=T}
lm <- lm(sound~temp, data=cricket)
lm.table <- augment(lm) #can visualize all the residuals in a table form
```

ggplot(lm.table, aes(x=.fitted, y=.resid))+
  geom_point(alpha=0.5)

### 3. Least Square Lines
Best way to have a linear regression line is to minimize the sum of squared residuals.

$Slope(b_1 = SD_y/SD_x * R)$

```{r eval=T, echo=T}
lm.sd <- lm.table %>% 
  summarize(sound.sd=sd(sound), temp.sd=sd(temp), cor=cor(sound, temp)) %>% 
  mutate(slope=(sound.sd/temp.sd)*cor) #Slope = 0.211
```
When we look at the **lm** model, the slope is also 0.211.
```{r eval=F, echo=T}
summary(lm)
```

### 4. Conditions for Linear Regression
**a.** Linearity (scatterplot + residual plot - residuals needs to be random)

**b.** Nearly normal residuals (histogram of residuals or QQ residual plot)

**c.** Constant variability (residual plot)

[Link](https://gallery.shinyapps.io/slr_diag/) for interactive regression diagnostic test. 

```{r eval=T, echo=T}
a <- ggplot(lm.table, aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype="dashed", color="red")+
  labs(title="Residuals vs Fitted Values", x="Fitted Values", y="Residuals")
b <- ggplot(lm.table, aes(x=.resid))+
  geom_density()+
  labs(title="Histogram of residuals", x="Residuals") #geom_density can also be added
c <- ggplot(lm.table, aes(sample=.resid))+
  stat_qq()+
  stat_qq_line()
grid.arrange(a, b, c, ncol=3)
```

### 5. Inference

* Hypothesis testing on the slope to identify if the explanatory variable is a significant predictor.

* Null hyp: H~0~ = 0 (no relationship). Alt hyp: H~1~ not 0 (yes relationship).

$t-stat = (pointestimate - null value) / SE$

```{r eval=T, echo=T}
summary(lm)
```
**t value** can be foudn by: (0.211 - 0) / 0.039 = 5.4

For 95% **confidence interval (CI)**: 0.211 +- *2.06* x 0.0387 = (0.13, 0.29)



***